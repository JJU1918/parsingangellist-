{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19dd65-b95c-4a76-b2b2-39535973e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries \n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e569b7-64f1-41c8-a836-16832291f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers= {\n",
    "    \"accept\": \"\",\n",
    "    \"user-agent\": \"\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_source_html(url):\n",
    "    \n",
    "    #options\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    #user-agent\n",
    "    options.add_argument(\"user-agent=\")\n",
    "\n",
    "    #disable webdriver\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    #chromedriver\n",
    "    driver = webdriver.Chrome(\n",
    "        executable_path=r\"\",\n",
    "        options=options\n",
    "    )\n",
    "\n",
    "    #selenium gains access to the site \n",
    "    try:\n",
    "        driver.get(url=url)\n",
    "        time.sleep(5)\n",
    "    \n",
    "\n",
    "        email_input = driver.find_element_by_name(\"email\")\n",
    "        email_input.clear()\n",
    "        email_input.send_keys(\"\")\n",
    "        time.sleep(10)\n",
    "    \n",
    "        password_input = driver.find_element_by_name(\"password\")\n",
    "        password_input.clear()\n",
    "        password_input.send_keys(\"\")\n",
    "        time.sleep(10)\n",
    "        password_input.send_keys(Keys.ENTER)\n",
    "    \n",
    "        time.sleep(10)\n",
    "    \n",
    "        join_link = driver.find_element_by_link_text(\"Find Syndicates to Join\").click()\n",
    "        time.sleep(7)\n",
    "        \n",
    "        first_client = driver.find_element_by_xpath(\"/html/body/div[1]/div/main/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div/div[2]/div/div/div/div[2]/div[2]/div/div[1]\").click()\n",
    "        time.sleep(5)\n",
    "        personal_page = driver.find_element_by_xpath(\"/html/body/div[4]/div/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div[1]/a\").click()\n",
    "        time.sleep(7)\n",
    "        name = driver.find_element_by_xpath(\"/html/body/div[1]/div[4]/div/div/div[2]/div[1]/div/div[2]/div/div/div[2]/div\").get_attribute(\"class\")\n",
    "        time.sleep(7)\n",
    "        print(name.text)\n",
    "        \n",
    "        #trying to find the info only with selenium\n",
    "        \n",
    "        #back_link = driver.execute_script(\"window.history.go(-1)\")\n",
    "        #time.sleep(7)\n",
    "        #for i in range (10):\n",
    "            #load_more_link = driver.find_element_by_xpath(\"//html/body/div[1]/div/main/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div/div[2]/div/button\").click()\n",
    "            #time.sleep(6)\n",
    "        #up_link = driver.find_element_by_tag_name('body').send_keys(Keys.CONTROL + Keys.HOME)  \n",
    "        \n",
    "        urls = []\n",
    "        for i in range(200):\n",
    "            first_client = driver.find_element_by_xpath(f'/html/body/div[1]/div/main/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div/div[2]/div/div/div/div[2]/div[{i+1}]/div/div[1]/div').click()\n",
    "            time.sleep(5)\n",
    "            links = driver.find_element_by_xpath(\"/html/body/div[4]/div/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div[1]/a\").get_attribute(\"href\") \n",
    "            time.sleep(3)\n",
    "            back = driver.find_element_by_xpath(\"/html/body/div[4]/div/div/div/div[1]/div\").click()\n",
    "            time.sleep(5) \n",
    "            urls.append(links)\n",
    "            print(links)\n",
    "        with open(\"three.txt\", \"w\") as file:\n",
    "            for url in urls:\n",
    "                file.write(f\"{url}\\n\")\n",
    "    \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    finally:\n",
    "        driver.close()\n",
    "        driver.quit()\n",
    "        \n",
    "\n",
    "def main():\n",
    "    get_source_html(url=\"https://angel.co/v/login\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cac07b-dcb1-462f-a6df-4de219c19bca",
   "metadata": {},
   "source": [
    "### Collecting the information about the syndicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824bed7-6f18-4715-b8fa-9a63646576d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  'Connection': 'keep-alive',\n",
    "  'Upgrade-Insecure-Requests': '1',\n",
    "  'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.152 Safari/537.36',\n",
    "  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "  'Sec-GPC': '1',\n",
    "  'Sec-Fetch-Site': 'same-origin',\n",
    "  'Sec-Fetch-Mode': 'navigate',\n",
    "  'Sec-Fetch-User': '?1',\n",
    "  'Sec-Fetch-Dest': 'document',\n",
    "  'Referer': 'https://www.angellist.com',\n",
    "  'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',\n",
    "}\n",
    "\n",
    "\n",
    "def get_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        urls = file.readlines()\n",
    "        clear_urls_list = []\n",
    "        for url in urls:\n",
    "            url = url.strip()\n",
    "            clear_urls_list.append(url)\n",
    "    result_list = [] \n",
    "    urls_count = len(clear_urls_list)\n",
    "    count = 1\n",
    "        #print(clear_urls_list)\n",
    "        #urls = [url.strip() for url in file.readlines()]\n",
    "        #print(urls)\n",
    "\n",
    "    for url in clear_urls_list:\n",
    "        t = time.time() # get the current time\n",
    "        resp = requests.get(url = url, headers=headers)\n",
    "        response_time = time.time() - t\n",
    "        time.sleep(round(response_time * 3)) # make a delay (round it up to a whole number)\n",
    "        \n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        \n",
    "        #syndicate name\n",
    "        item_investors = soup.find_all('div', {'class': 'u-fontSize48 u-fontSize30SmOnly'})\n",
    "        for tag1 in item_investors:\n",
    "            investors = tag1.get_text().strip()\n",
    "            print(investors)\n",
    "        \n",
    "        #typical investment\n",
    "        item_typical_investments = soup.find_all('div', {'class': 'u-fontSize24'})[0]\n",
    "        for tag2 in item_typical_investments:\n",
    "            investments = tag2.get_text().strip()\n",
    "            print(investments)\n",
    "        \n",
    "        #last deals\n",
    "        item_last_deals = soup.find_all('div', {'class': 'u-fontSize24'})[1]\n",
    "        for tag3 in item_last_deals:\n",
    "            deals = tag3.get_text().strip()\n",
    "            print(deals)\n",
    "            \n",
    "        #lps\n",
    "        item_lps = soup.find_all('div', {'class': 'u-fontSize24'})[2]\n",
    "        for tag4 in item_lps:\n",
    "            lps = tag4.get_text().strip()\n",
    "            print(lps)\n",
    "        \n",
    "        #print(investors + '  ' + investments + '  ' +  deals + '  ' + lps)\n",
    "        \n",
    "        #save to json\n",
    "        result_list.append(\n",
    "            {\n",
    "                \"investors\": investors,\n",
    "                \"investments\": investments,\n",
    "                \"deals\": deals,\n",
    "                \"lps\": lps\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        #avoid the block\n",
    "        time.sleep(random.randrange(2, 5))\n",
    "    \n",
    "        if count%10 == 0:\n",
    "            time.sleep(random.randrange(10, 15))\n",
    "    \n",
    "        print(f\"[+] Processed: {count}/{urls_count}\")\n",
    "    \n",
    "        count += 1\n",
    "    \n",
    "    with open(\"namefile.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(result_list, file, indent=4, ensure_ascii = False)\n",
    "    \n",
    "    return \"[INFO] Data collected successfully\"\n",
    "        \n",
    "    \n",
    "def main():\n",
    "    get_data(file_path=r\"where the Windows/OS finds the links.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
